# Embedding（嵌入模型）

## 定义

**Embedding = 把一段文本转换成一个固定维度的数字向量**（例如 768 维、1536 维、3072 维）。

可以理解成：把一段文本"压缩"成一个固定长度的数字数组（向量），让计算机用距离/角度来判断"语义像不像"。

## 核心概念

### 维度（Dimension）

- **定义**：Embedding 模型输出的向量长度，一个由浮点数组成的数组
- **常见维度**：768、1024、1536（OpenAI）、4096
- **作用**：决定语义表达的细腻程度和数据库的存储大小

维度越高，能捕捉的语义细节通常越丰富，但存储成本和检索成本也会增加。

### Token vs 维度

**Token（输入量）**
- 文本被切分成的最小单位
- 决定 API 调用费用和模型的上下文窗口限制
- 比喻：邮局寄信的信纸字数

**维度（输出量）**
- 输出向量的长度，固定不变
- 决定向量数据库的存储费用
- 比喻：固定规格的邮政编码

**关键点**：
- 维度是固定的：无论输入是 "Hello"（1 Token）还是长文档，输出永远是 1536 维
- 信息压缩：输入过长会导致信息丢失，语义被稀释

## 模型承载能力（Token 限制）

大部分 Embedding 模型在输入超过限制时，**并不会报错，而是默认截断（Truncate）**，这会导致"静默失败"——即模型丢弃了超出部分的文本。

### 常见模型限制

| 模型 | 最大 Token 数 |
|------|--------------|
| OpenAI text-embedding-3-small | 8191 |
| BERT 类模型（BGE, E5, BERT） | 512 |
| Jina-Embeddings-v2 | 8192 |
| Google text-embedding-004 | 768 |

### 防止截断

需要对比"输入文本的 Token 数量"与"模型的最大上下文窗口"，确保输入在限制范围内。

## 向量检索原理

向量检索能"按语义找相似"的前提：

1. 文档入库时用 Embedding 模型将文档变成向量
2. 查询时也必须用**同一个 Embedding 模型**把 query 变成向量
3. 两边在同一个向量空间里，距离才有意义

**关键**：
- 入库向量的维度必须和查询向量的维度一致
- 最好来自同一个 Embedding 模型，否则"距离"没意义

## Embedding 提供方选择

### 云端 API（按量付费）

| 提供方 | 模型 | 维度 | 特点 |
|--------|------|------|------|
| OpenAI | text-embedding-3-small | 1536 | 稳定、按量计费 |
| Google | text-embedding-004 | 768 | 性价比高 |
| Cohere | embed-v3 | 1024 | 多语言支持 |

### 本地模型（免费）

使用本地模型生成 Embedding，再写入向量数据库：

- **Ollama** + `nomic-embed-text` / `bge-m3` / `bge-large`
- 优点：不需要 API Key、不按量付费、数据不出本机
- 代价：需要本机/服务器算力，首次部署稍麻烦

## 成本估算

### Embedding 成本（一次性/增量）

- 文档越多、chunk 越多、更新越频繁，花费越高
- 工程策略：先全量跑通，后面再做增量更新

### 向量数据库存储成本

- 近似估算：向量存储量 ≈ `条数 × 维度 × 4 bytes`（float32）
- 10 万条 × 1536 × 4 ≈ 614MB（还不算索引开销）
- 索引（HNSW 等）会增加额外内存占用

## 与 LLM 的关系

**Codex/Gemini 都只是"会话模型（LLM）"**，它们是否能用向量数据库取决于是否给它们同一套"检索工具链"：

- 共同点：都可以"先 embedding → 再向量搜索 → 再把 topK chunks 注入提示词"
- 差异点：它们调用的工具/运行环境不同
- 底层必须统一：**同一个 embedding 提供方/模型 + 同一个向量维度 + 同一个 collection**

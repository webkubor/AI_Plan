## `nomic-embed-text`



- 我们把文字通过 markdown 语法写在项目中
- AI 是那个负责把文字变成数字（坐标）的翻译官。

#### 768 维的向量长度 (Dimension)

   * 比喻： 假设你要在地图上描述一个人的位置。
       * 2 维 (x, y)：你只能说“他在经度 100，纬度 30”。这很模糊。
       * 3 维 (x, y, z)：你能加上高度，“他在经度 100，纬度 30，海拔 500米”。精确了一点。
       * 768 维：你可以描述“经度、纬度、海拔、时间、温度、心情、衣服颜色、甚至他昨天吃没吃早饭...”。
   * 实际意义： 维度越高，描述一个句子（语义）的“分辨率”就越高，能捕捉到的细微差别就越多。
       * 768 是个“甜点位” (Sweet Spot)。它比 384 维（如旧版 BERT）更准，又比 1536 维（如 OpenAI）更省资源、算得更快。对于本地跑的 RAG 来说，它是精度与速度的完美平衡。

 8k 上下文长度 (Context Length)

   * 比喻： 这是 AI 的“瞬时记忆容量”或“一口气能读多少字”。
       * 512 (旧模型)：AI 像金鱼，只能读一句话。如果你丢给它一段代码，它读到第 10 行就忘了第 1 行是啥了。
       * 8k (nomic-embed-text)：AI 能一口气读完一篇长论文或一个完整的代码文件。
   * 实际意义：
       * 切片 (Chunking)：你可以放心把一大段 vibe_rules.md (假设 5000 字) 整个丢给它，不用切得稀碎。
       * 跨段理解：它能理解文章开头和结尾的关联。比如开头定义了变量 X，结尾用了 X，8k 模型能把它们关联起来生成一个向量；而短模型可能根本看不到这种联系。
       
       

Embedding 模型 配置如下：

   * 默认模型: `nomic-embed-text`
   * 运行环境: 通过 Ollama 本地运行。
   * 配置方式: 脚本中使用了 process.env.OLLAMA_MODEL || 'nomic-embed-text'。这意味着除非你手动设置了环境变量 OLLAMA_MODEL，否则它将默认使用 nomic-embed-text。

  `nomic-embed-text` 是目前本地 RAG 非常主流的选择，它具有 768 维的向量长度，且对长文本（8k context length）的支持非常出色。



 nomic-embed-text 就像是一个“记忆力好（能读长文）且观察入微（描述精准）”的速记员。这对于构建技术文档（通常很长）和代码库（依赖上下文）的知识库来说，是绝佳的选择。


## 从openCode 到 AionUI 解决了什么问题

> 从openCode 到 AionUI,**运行在终端里的自动化包工头** 到**能接各种 AI 模型的**“**超级浏览器**”。
>
> 参考文: https://juejin.cn/post/7597346583114481690?share_token=FCC1DB00-A509-4445-93AF-A033323439C2



#### PlaywrightMcp

> 关于自动化测试
>
> 参考文: https://www.npmjs.com/package/@playwright/mcp
>
> ```js
> {
>   "mcpServers": {
>     "playwright": {
>       "command": "npx",
>       "args": [
>         "@playwright/mcp@latest"
>       ]
>     }
>   }
> }
> ```

### 本地模型的管理

AionUI 对接 Ollama 的方式

> 用“Custom Platform（OpenAI 兼容接口）”把 Ollama 的 OpenAI 兼容地址填进去即可。下面是最短可用流程：

**步骤**

1. 确认 Ollama 本地已启动并拉取模型
   比如先拉一个模型并确保服务在跑（默认端口 11434）。([docs.ollama.com](https://docs.ollama.com/openai?utm_source=openai))
2. 在 AionUi 里配置 Custom Platform

- 进入 Settings → Model Settings → Add Model

- Platform 选 “Custom Platform”

- Base URL 填 Ollama 的 OpenAI 兼容地址

- API Key 随便填一个字符串（Ollama 会忽略，但 AionUi 要求有值）

- 选择/填写模型名（要和你 `ollama pull` 的模型名一致）
  AionUi 的配置说明见 LLM Configuration（Custom Platform 的 Base URL + API Key），并且官方明确支持本地 Ollama，用法是设置本地 API 地址（示例 `http://localhost:11434/v1`）。

  

`````
from openai import OpenAI

client = OpenAI(
    base_url='http://localhost:11434/v1/',
    api_key='ollama',  # required but ignored
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            'role': 'user',
            'content': 'Say this is a test',
        }
    ],
    model='gpt-oss:20b',
)
print(chat_completion.choices[0].message.content)
`````

#### 界面配置如下

![screenshot-20260121-103013](https://raw.githubusercontent.com/webkubor/upic-images/main/uPic/2026/01/screenshot-20260121-103013-20260121162213938.png)
